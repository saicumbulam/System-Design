- [stream processing steps](#stream-processing-steps)
  - [continuous operator model](#continuous-operator-model)
- [Architecture of Spark Streaming: Discretized Streams](#architecture-of-spark-streaming-discretized-streams)
  - [Benefits of Discretized Stream Processing](#benefits-of-discretized-stream-processing)
- [Unification of batch, streaming and interactive analytics](#unification-of-batch-streaming-and-interactive-analytics)
- [Performance](#performance)

Apache Spark has provided an unified engine that natively supports both batch and streaming workloads. 

Stream Processing Architectures – The Old and the New

## stream processing steps
Modern distributed stream processing pipelines execute as follows:

1. Receive streaming data from data sources (e.g. live logs, system telemetry data, IoT device data, etc.) into some data ingestion system like Apache Kafka, Amazon Kinesis, etc.
2. Process the data in parallel on a cluster.
3. Output the results out to downstream systems like HBase, Cassandra, Kafka, etc.

### continuous operator model
To process the data, most traditional stream processing systems are designed with a continuous operator model, which works as follows:

There is a set of worker nodes, each of which run one or more continuous operators.
Each continuous operator processes the streaming data one record at a time and forwards the records to other operators in the pipeline.
There are “source” operators for receiving data from ingestion systems, and “sink” operators that output to downstream systems.

![picture 17](../images/493625949d40bb88d0a922cd05c9cccf99b0f2d8da6ec6a590995e893a29ddff.png)  

## Architecture of Spark Streaming: Discretized Streams

To address the problems like straggler recovery, Uneven allocation of the processing load, Spark Streaming uses a new architecture called discretized streams.

1. Instead of processing the streaming data one record at a time, Spark Streaming discretizes the streaming data into tiny, sub-second micro-batches. Meaning, Spark Streaming’s Receivers accept data in parallel and buffer it in the memory of Spark’s workers nodes. 
2. Then the latency-optimized Spark engine runs short tasks to process the batches and output the results to other systems. Note that unlike the traditional continuous operator model, where the computation is statically allocated to a node, Spark tasks are assigned dynamically to the workers based on the locality of the data and available resources. This enables both better load balancing and faster fault recovery.

In addition, each batch of data is a Resilient Distributed Dataset (RDD), which is the basic abstraction of a fault-tolerant dataset in Spark. This allows the streaming data to be processed using any Spark code or library.

![picture 18](../images/d3dd9af10f8a8ccc2ce4569123835db558c0e410a35a8389ba6da9bf6c3fef61.png)  


### Benefits of Discretized Stream Processing

**Dynamic load balancing**

Dividing the data into small micro-batches allows for fine-grained allocation of computations to resources. some workers will process a few longer tasks, others will process more of the shorter tasks.

![picture 19](../images/fa6c5ce80435772d57e94c3bfe824d5d13dce52395091f570e248125cf1e8051.png)  


**Fast failure and straggler recovery**

In case of node failures, failed tasks can be relaunched in parallel on all the other nodes in the cluster, thus evenly distributing all the recomputations across many nodes, and recovering from the failure faster than the traditional approach.

![picture 20](../images/bf2d26f201fbc9af67c63cbb7391833fd68e92e11b2d5c03c996ca20038df96c.png)  


## Unification of batch, streaming and interactive analytics

1. **distributed stream**: 
Each batch of streaming data is represented by an RDD, which is Spark’s concept for a distributed dataset. Therefore a DStream is just a series of RDDs. This common representation allows batch and streaming workloads to interoperate seamlessly. Users can apply arbitrary Spark functions on each batch of streaming data: for example, it’s easy to join a DStream with a precomputed static dataset (as an RDD).

``` scala
// Create data set from Hadoop file
val dataset = sparkContext.hadoopFile("file")
// Join each batch in stream with the dataset
kafkaDStream.transform { batchRDD =>
  batchRDD.join(dataset).filter(...)
}
```

1. RDDs generated by DStreams can be converted to DataFrames (the programmatic interface to Spark SQL), and queried with SQL. 

## Performance

1. In terms of latency, Spark Streaming can achieve latencies as low as a few hundred milliseconds. 
2. **Backpressure –** Streaming workloads can often have bursts of data (e.g. sudden spike in tweets during the Oscars) and the processing system must be able to handle them gracefully. Adding backpressure mechanisms that allow Spark Streaming dynamically control the ingestion rate for such bursts. This feature represents joint work between us at Databricks and engineers at Typesafe.